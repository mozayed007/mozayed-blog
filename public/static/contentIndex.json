{"about/index":{"title":"About Me","links":[],"tags":["Info"],"content":""},"blog/Resume-Tips":{"title":"Resume Tips","links":[],"tags":["Resume","Tips"],"content":"Zero Experience Resume Tips From a Big Tech Hiring Manager\n\nThis note is a markdown formatted summary of the YouTube video ‚ÄùZero Experience Resume Tips From a Big Tech Hiring Manager‚Äù  by Exaltitude designed to help recent graduates with no full-time engineering experience craft compelling resumes for landing their dream job in the tech industry.\nCrafting a Winning Resume: Key Takeaways\n\nATS-Friendly Formatting: ¬†\n\nSimple &amp; Readable: Utilize a professional, easy-to-read font like Arial or Calibri. ¬†\nConsistency is Key: Maintain consistent formatting throughout your resume, including font size, margins, and spacing.\nStandardized Dates: Use a consistent date format (e.g., YYYY-MM-DD) for all projects and experiences.\n\n\nStructure &amp; Content: ¬†\n\nEssential Sections: Focus on core sections like contact information (ensure it‚Äôs professional and reachable), skills (tailored to the job description), education (including relevant coursework and achievements), work experience (including volunteer work), and impactful personal projects. ¬†\nOptional Objective Section: A concise summary statement (2-3 sentences) highlighting your skills and career goals can be impactful for highly competitive roles.\n\n\nKeyword Magic: Research relevant keywords from the job description and strategically integrate them throughout your resume, particularly in the skills section and within bullet points describing your experience.\n\nHighlighting Your Skills and Experience:\n\nEducation Excellence: Spell out college names in full and include earned degrees and relevant coursework. List relevant coursework that showcases technical skills applicable to the target job.\nQuantify Your Impact: Whenever possible, quantify your accomplishments using metrics like time saved, increased efficiency, or user engagement improvements to demonstrate the impact of your work.\nAction Verbs Drive Results: Start your bullet points with strong action verbs that showcase your initiative and achievements (e.g., ‚ÄúDeveloped,‚Äù ‚ÄúImplemented,‚Äù ‚ÄúSpearheaded‚Äù).\nClarity &amp; Conciseness: Avoid unnecessary jargon or overly elaborate sentences. Focus on clear, concise language that effectively communicates your skills and experience.\n\nPersonal Projects: Showcase Your Passion (and Skills):\n\nQuality over Quantity: Focus on impactful projects that demonstrate your relevant skills and problem-solving abilities. Don‚Äôt just list a bunch of projects; choose a few that showcase your best work.\nOpen Source Contribution: Consider contributing to open-source projects to gain valuable experience and demonstrate your collaborative skills. Link to your GitHub profile if applicable.\nTailor to the Role: Align your personal projects with the target job description whenever possible. This shows initiative and genuine interest in the specific field.\n\nBeyond the Video: Additional Tips for Success\n\nProofread Me: Carefully proofread your resume for any typos or grammatical errors. Consider having a trusted friend or career counselor review it for clarity and impact.\nAction Verbs Throughout: Use strong action verbs throughout your resume to showcase your initiative and accomplishments.\nTailor It Every Time: Don‚Äôt submit a generic resume. Tailor your resume to each specific job application by highlighting relevant skills and experiences mentioned in the job description.\nLeverage Online Resources: Utilize online resume builders or templates to get a head start, but remember to personalize them with your unique experience and skills.\n\nBonus Tip: Stand Out From The Crowd\nConsider including a link to a relevant portfolio showcasing your projects or code samples. This can be especially helpful for visual or code-heavy fields like web development or UX design.\nBy following these tips and crafting a well-structured, ATS-friendly resume that effectively showcases your skills and accomplishments, you‚Äôll be well on your way to impressing big tech hiring managers, even with no prior full-time engineering experience."},"blog/Saudi-Tech-Opportunities":{"title":"Emerging Opportunities - Saudi Arabia","links":["tags/LEAP24"],"tags":["LEAP24","Tech","Opportunities"],"content":"LEAP24: Tech Opportunities in Saudi Arabia ‚Äì The New Silicon Valley\n\n\nThere are multiple opportunities that are rising talents dreaming to kick start their journey in Tech.\nIn this summary, I‚Äôm focusing on the opportunities that might interest early birds like me who are looking for opportunities to start their careers and what a perspective for needed talents could be.\n\nIntroduction\nSaudi Arabia is emerging as a hub for technology and innovation, akin to the famed Silicon Valley. LEAP 2024, a digital Davos, brings together global tech enthusiasts, change-makers, and visionaries. Here‚Äôs a concise summary of the event, focusing on Day 1 and Day 2.\nDay 1 -  Unveiling Cloud Service Providers and Data Centers\nCloud Service Providers\n\n\nAWS SA Region: Amazon Web Services (AWS) continues to expand its presence in Saudi Arabia, offering scalable cloud solutions.\n\n\nIBM Software Lab: IBM‚Äôs cutting-edge software lab provides a platform for innovation and collaboration.\n\n\nServiceNow Data-center: ServiceNow‚Äôs data center infrastructure supports critical business processes.\n\n\nData-volt Sustainable Data-center: A sustainable data center solution, emphasizing energy efficiency and environmental responsibility.\n\n\n\n\n\nAramco‚Äôs Innovations\n\nSAIL:\n\nSAIL (Saudi Accelerated Innovation Lab): Aramco‚Äôs lab fosters rapid innovation, bridging the gap between research and practical applications.\nARAMCO METABRAIN (Industrial LLM): Leveraging machine learning and AI, Aramco aims to enhance industrial processes.\n\n\n\nDay 2 - NTDP‚Äôs Vision for Startups\nNTDP (Next Technology Development Program)\n\nThey unveiled five groundbreaking products, collaborating with key entities:\n\nSDAIA\nMCIT\nKAUST\n\n\nProducts\n\nThey Focus on AI &amp; Cloud to boost startups as a main vision:\n\n\nFuel: Designed to support VC funds investing in deep tech and emerging tech! üëè\nAIM (Artificial Intelligence Mission): An AI innovation platform providing access to R&amp;D infrastructure and development. ü¶æ\nFundSwift: Providing bridge financing for startups, accelerating early-stage startup growth through swift financing! üìà\nTransform+: Designed to bridge the technology gap for startups to adapt to the cloud! ‚òÅÔ∏è\nSourceTech: Incentivizing IT outsourcing firms to establish themselves within Saudi Arabia! üèóÔ∏è\n\n\nThese are the 5 different products that NTDP officially announced at#LEAP24.\n\n\n\n\nIn-Demand Tech Roles in Saudi Arabia\nAs Saudi Arabia positions itself as the new Silicon Valley, several tech roles are expected to be in high demand. Whether you‚Äôre an early-career talent or an experienced professional, consider exploring these opportunities as an example list with hints from the LEAP summary:\n\nCloud Solutions Architect: Designing and implementing scalable cloud solutions.\nSoftware Engineer (IBM Software Lab): Collaborating on cutting-edge software projects.\nData Center Specialist (ServiceNow Data-center): Managing critical data center infrastructure.\nSustainability Analyst (Data-volt Sustainable Data-center): Focusing on energy-efficient data centers.\nInnovation Researcher (SAIL - Saudi Accelerated Innovation Lab): Exploring rapid innovation opportunities.\nMachine Learning Engineer (ARAMCO METABRAIN): Leveraging AI for industrial enhancements.\nVenture Capital Analyst (Fuel): Supporting deep tech and emerging tech startups.\nAI Researcher (AIM - Artificial Intelligence Mission): Working on R&amp;D projects in AI.\nFinancial Analyst (FundSwift): Facilitating swift financing for early-stage startups.\nCloud Adoption Specialist (Transform+): Assisting startups in transitioning to cloud technologies.\nBusiness Development Manager (SourceTech): Encouraging IT outsourcing firms to establish in Saudi Arabia.\nCloud Engineer: Assisting in cloud infrastructure management and deployment.\nData Analyst: Analyzing data trends and patterns to inform business decisions.\nFront-End Developer: Building user interfaces and enhancing user experiences.\nQuality Assurance (QA) Tester: Ensuring software quality through testing and bug identification.\nTechnical Support Specialist: Providing assistance to users and troubleshooting technical issues.\nDevOps Engineer: Learning about continuous integration, deployment, and automation.\nCybersecurity Analyst: Monitoring and safeguarding systems against security threats.\nAI/ML Engineer: Exploring machine learning models and algorithms.\nData Scientist: Analyzing complex data, building models, and extracting insights.\n¬†Cybersecurity Engineer: Focusing on network security infrastructure and threat monitoring.\nFull-Stack Developer: Gaining experience in both front-end and back-end development.\nIT Specialist: Participating in real-world projects and learning from experienced professionals.\n\nConclusion\n\nThese announcements at#LEAP24 signify Saudi Arabia‚Äôs commitment to fostering tech talent, innovation, and global collaboration. Stay tuned for more exciting developments! üöÄ\n"},"blog/index":{"title":"Blogs, Notes and Posts","links":[],"tags":["Technical","Writing","Blog"],"content":""},"index":{"title":"Welcome to My Space","links":[],"tags":[],"content":"Welcome to MoZayed‚Äôs Blog\nüë®‚Äçüíª About Me\n\nüëÄ I‚Äôm interested in: AI, Machine Learning, Cybersecurity, and Strategic Development Techs (Gartner).\nüèì Hobbies: Anime, MMORPGs, TFT, FPS üéÆ.\nüå± I‚Äôm currently learning: Machine Learning, AI, NLP, CV, and (Cybersecurity (self study)).\nüõ† I‚Äôve experience in Technical Support, Freelancing, Online Tutoring, Machine / Deep Learning, NLP, Computer Vision and LLMs.\nüè´ Senior CIE Undergraduate at UST Zewail City.\n\nüìö Blog Posts\nHere you‚Äôll find a collection of my thoughts, experiences, and learnings. I write about everything I take notes about whether experiences I meet or content I learn from regarding tech, and occasionally about my hobbies and interests.\nüì´ Contact Me\nFeel free to reach out to me via email: s-mohamedzayed@zewailcity.edu.eg or LinkedIn.\nYou can also support me on Ko-fi.\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n"},"projects/Anime-Recommender/Code-Structure":{"title":"Code Structure","links":[],"tags":["NLP","project","recommender","content-based-filtering"],"content":"Basic Structure\nOverview\nThis project aims to build an anime recommendation system using LlamaIndex, a language model, and Streamlit. The system will recommend similar anime series based on semantic similarity of descriptions and categorical features.\nCode Structure\nThe project is divided into several parts, each contained in its own Python file:\n\n\nlanguage_model.py: This file contains the code for loading and using the language model. It uses the transformers library from Hugging Face to load a pre-trained model and compute embeddings for text.\nfrom transformers import AutoTokenizer, AutoModel\n \nclass LanguageModel:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n \n    def encode(self, text):\n        input_ids = self.tokenizer.encode(text, return_tensors=&#039;pt&#039;)\n        embeddings = self.model(input_ids)[0].mean(1)\n        return embeddings.detach().numpy()\n\n\nllama_index.py: This file contains the code for indexing the anime data using LlamaIndex. It loads the data from a file, computes the embeddings of the anime descriptions using the language model, and adds the anime to the index.\nfrom llama_index import LlamaIndex\n \nclass AnimeIndex:\n    def __init__(self, data_path, language_model):\n        self.index = LlamaIndex()\n        self.language_model = language_model\n        self.load_data(data_path)\n \n    def load_data(self, data_path):\n    # Load your data from the CSV or Parquet file\n    # For each anime, compute the embedding of its description using the language model\n    # Add the anime to the index using its embedding and categorical features\n \n    def query(self, text, k=10):\n# Compute the embedding of the query text using the language model\n# Query the index using the embedding and return the top k results\n\n\nrecommender.py: This file contains the code for the recommendation system. It uses the indexed data to generate recommendations based on the user‚Äôs input.\nclass Recommender:\n    def __init__(self, anime_index):\n        self.anime_index = anime_index\n \n    def recommend(self, text, k=10):\n        # Query the index using the text and return the top k results\n\n\nstreamlit_app.py: This file contains the code for the Streamlit app. It provides a user interface for the recommendation system.\nimport streamlit as st\nfrom src.recommendation.recommender import Recommender\n \n# Initialize your recommender\nrecommender = Recommender()\n \nst.title(&#039;Anime Recommender&#039;)\n \n# Get user input\ntext = st.text_input(&#039;Enter a description of an anime you like&#039;)\n \nif text:\n    # Get recommendations\n    recommendations = recommender.recommend(text)\n \n    # Display recommendations\n    for anime in recommendations:\n        st.write(anime)\n\n\nConclusion\nBuilding a recommendation system is a complex task that requires a good understanding of both the data and the tools used. This project provides a basic structure for such a system, but it might need to be modified according to specific requirements and the actual structure of the data. Also, it‚Äôs important to handle exceptions and edge cases in the code to make it robust and reliable."},"projects/Anime-Recommender/Data-Updates":{"title":"Data Overview","links":[],"tags":["NLP","project","recommender","content-based-filtering"],"content":"Data Management\nIf your data is updated every three months, you can handle this in LlamaIndex by re-indexing your data. Here‚Äôs a high-level overview of how you might do this:\n\n\nLoad New Data: Load the new data from your data source. This could be a CSV file, a Parquet file, a database, or any other source.\n\n\nCompute Embeddings: Use your language model to compute the embeddings of the anime descriptions in the new data.\n\n\nUpdate the Index: Add the new data to the LlamaIndex. If an anime already exists in the index (based on some unique identifier like an ID or name), update its entry with the new data. Otherwise, add a new entry for the anime.\n\n\nHere‚Äôs a basic example of how you might implement this in code:\n```python\ndef update_index(self, new_data_path):\n    # Load the new data\n    new_data = self.load_data(new_data_path)\n\n    for anime in new_data:\n        # Compute the embedding of the anime description\n        embedding = self.language_model.encode(anime.description)\n\n        # If the anime already exists in the index, update its entry\n        if self.index.exists(anime.id):\n            self.index.update(anime.id, embedding, anime.categorical_features)\n        # Otherwise, add a new entry for the anime\n        else:\n            self.index.add(anime.id, embedding, anime.categorical_features)\n````\n\nPlease note that this is a basic example. You‚Äôll need to modify it according to your specific requirements and the actual structure of your data. Also, remember to handle exceptions and edge cases in your code to make it robust and reliable.\nRecord Management\nRecord management is a crucial aspect of any data-driven system. It helps keep track of when and what data was added or updated in the system. Although LlamaIndex does not provide built-in record management features, you can implement your own system based on your needs.\nSimple Record Management\nFor a simple record management system, you could keep a log file where each line records an operation on the index. Here‚Äôs an example of what each line in the log file might look like:\n```code\n2024-03-01 12:34:56, ADD, anime123, ‚ÄúAnime 123 description‚Äù \n2024-03-01 12:35:07, UPDATE, anime456, ‚ÄúUpdated Anime 456 description‚Äù\n```\n\nEach line contains a timestamp, the operation (ADD or UPDATE), the ID of the anime, and the new description of the anime.\nAdvanced Record Management\nFor a more advanced record management system, you could use a separate database to keep track of the operations. This would allow you to run complex queries on your records and could be more efficient if you have a large number of operations.\nHere‚Äôs an example of how you might structure your database:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimestampOperationAnime IDDescription2024-03-01 12:34:56ADDanime123‚ÄúAnime 123 description‚Äù2024-03-01 12:35:07UPDATEanime456‚ÄúUpdated Anime 456 description‚Äù\nEach row in the database represents an operation on the index.\nRemember, the choice between a simple and advanced record management system depends on your specific needs and the scale of your project."},"projects/Anime-Recommender/General-Plan":{"title":"General Plan","links":[],"tags":["NLP","project","recommender","content-based-filtering"],"content":"For my use case, I would recommend using LlamaIndex. Here‚Äôs why:\n\n\nSemantic Similarity: LlamaIndex is designed to work with Language Models, which are excellent at understanding semantic similarity. You can use it to index your anime descriptions and then retrieve similar animes based on the semantic similarity of their descriptions.\n\n\nCategorical Features: While LlamaIndex primarily deals with text data, you can also use categorical features in your recommendation system. You can create a hybrid recommendation system that uses both the semantic similarity of the descriptions and the categorical features of the animes.\n\n\nData Format: LlamaIndex provides data connectors that can ingest data from various sources. So, whether your data is in CSV or Parquet format should not be a problem.\n\n\nIntegration with Streamlit: LlamaIndex can be used in conjunction with Streamlit to build a search bar interface. You can use Streamlit to take user input, pass it to your LlamaIndex-based recommendation system, and then display the top K similar animes.\n\n\nRemember, while LlamaIndex can help you build the recommendation system, I‚Äôll also need a language model to compute the semantic similarity of the anime descriptions.\nI might want to consider models like BERT or GPT-3 for this purpose.üòä"},"projects/Anime-Recommender/Recommendation-Algorithm":{"title":"Recommendation Plan","links":[],"tags":["NLP","project","recommender","content-based-filtering"],"content":"Anime Recommendation Algorithm\nThis document outlines the design of an anime recommendation algorithm that leverages semantic similarity and content-based filtering. The algorithm uses LlamaIndex for semantic similarity computations and a vector database for efficient similarity search.\n1. Preprocessing\nThe first step is to clean and preprocess the anime synopsis data. This might involve removing stop words, stemming, and tokenization.\ndef preprocess(synopses):\n    # Implement your preprocessing steps here\n    return preprocessed_synopses\n2. Vectorization\nNext, we use a language model (like BERT, Word2Vec, or FastText) to convert the preprocessed synopsis into vector representations. These vectors capture the semantic meaning of the synopsis.\nfrom sentence_transformers import SentenceTransformer\n \nmodel = SentenceTransformer(&#039;all-MiniLM-L6-v2&#039;)\nsynopsis_vectors = model.encode(preprocessed_synopses)\n3. Semantic Similarity with LlamaIndex\nWe use LlamaIndex‚Äôs SemanticSimilarityEvaluator to compute the semantic similarity between the query anime‚Äôs synopsis and all other anime synopses. This will give you a list of animes that are semantically similar to the query anime based on their synopses.\nfrom llama_index.core.evaluation import SemanticSimilarityEvaluator\n \n# Initialize the SemanticSimilarityEvaluator\nevaluator = SemanticSimilarityEvaluator()\n \n# Define an async function to compute semantic similarity\nasync def compute_similarity(query_synopsis, reference_synopsis):\n    # Use the SemanticSimilarityEvaluator to compute the semantic similarity score\n    result = await evaluator.aevaluate(response=query_synopsis, reference=reference_synopsis)\n    return result.score\n \n# Compute semantic similarity scores for all animes\nsemantic_similarity_scores = np.array([compute_similarity(query_synopsis, reference_synopsis) for reference_synopsis in preprocessed_synopses])\n4. Vector Database for Efficient Similarity Search\nA vector database, like Faiss from Facebook AI or Annoy from Spotify, allows you to efficiently search for vectors that are most similar to a given query vector. These libraries use Approximate Nearest Neighbor (ANN) search, which is a method to find the points in a dataset that are closest to a given point, approximately. This is much faster than comparing the query to every single point in the dataset.\nfrom llama_index.vector_store import VectorStore\n \n# Initialize the VectorStore\nvector_store = VectorStore()\n \n# Add the vectors to the VectorStore\nfor i, vector in enumerate(synopsis_vectors):\n    vector_store.add_vector(i, vector)\n \n# Define a function to get the top K most semantically similar animes\ndef get_semantic_recommendations(query_title, k=10):\n    # Find the index of the query anime\n    query_index = anime_data[anime_data[&#039;title&#039;] == query_title].index[0]\n \n    # Get the indices of the top K most semantically similar animes\n    semantic_indices = vector_store.query(synopsis_vectors[query_index], top_k=k)\n \n    return anime_data.iloc[semantic_indices]\n5. Content-Based Filtering\nFor each categorical feature of the anime (like genre, director, etc.), we compute similarity scores between the query anime and all other animes. We might use techniques like one-hot encoding or TF-IDF followed by cosine similarity.\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n \ntfidf = TfidfVectorizer()\nfeature_matrix = tfidf.fit_transform(anime_data[&#039;categorical_feature&#039;])\nfeature_similarity = cosine_similarity(feature_matrix)\n6. Combining Recommendations\nFinally, we combine the recommendations from the semantic similarity and content-based filtering. We might simply take the union of the top K recommendations from each, or we could use a more sophisticated approach like rank fusion.\ndef recommend_anime(query_title, k=10):\n    # Find the index of the query anime\n    query_index = anime_data[anime_data[&#039;title&#039;] == query_title].index[0]\n \n    # Compute semantic similarity\n    semantic_indices = get_semantic_recommendations(query_title, k)\n \n    # Compute content-based similarity\n    content_indices = np.argsort(feature_similarity[query_index])[-k:]\n \n    # Combine the recommendations\n    combined_indices = np.union1d(semantic_indices, content_indices)\n \n    return anime_data.iloc[combined_indices]\n\nThis is a more detailed version of the algorithm with a focus on the usage of LlamaIndex for semantic similarity and a vector database for efficient similarity search. There‚Äôs a lot of room for improvement."},"projects/Anime-Recommender/language-model":{"title":"Language model role","links":[],"tags":["NLP","project","recommender","content-based-filtering"],"content":"language_model.py\nThe language_model.py file is responsible for all operations related to the language model. This includes loading the model and computing embeddings for text.\nfrom transformers import AutoTokenizer, AutoModel\n \nclass LanguageModel:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n \n    def encode(self, text):\n        input_ids = self.tokenizer.encode(text, return_tensors=&#039;pt&#039;)\n        embeddings = self.model(input_ids)[0].mean(1)\n        return embeddings.detach().numpy()\nllama_index.py\nThe llama_index.py file uses the LanguageModel class from language_model.py to compute the embeddings of the anime descriptions when loading the data into the LlamaIndex.\nfrom src.models.language_model import LanguageModel\n \nclass AnimeIndex:\n    def __init__(self, data_path, model_name):\n        self.index = LlamaIndex()\n        self.language_model = LanguageModel(model_name)\n        self.load_data(data_path)\n \n    def load_data(self, data_path):\n        # Load your data from the CSV or Parquet file\n        # For each anime, compute the embedding of its description using the language model\n        # Add the anime to the index using its embedding and categorical features\nrecommender.py\nThe recommender.py file uses the LanguageModel class to compute the embedding of the user‚Äôs input when generating recommendations.\nclass Recommender:\n    def __init__(self, anime_index):\n        self.anime_index = anime_index\n \n    def recommend(self, text, k=10):\n        # Compute the embedding of the query text using the language model\n        # Query the index using the embedding and return the top k results\nPlease note that these are basic examples. You‚Äôll need to modify them according to your specific requirements and the actual structure of your data. Also, remember to handle exceptions and edge cases in your code to make it robust and reliable."},"projects/Image-Classifier-AWS/Data-Overview":{"title":"Dataset Overview","links":[],"tags":["computer-vision","project","recommender","content-based-filtering","transfer-learning","image-classification","flowers","Dataset"],"content":"Flowers 102 Dataset Overview\n\nIntroduction\nThe Flowers 102 dataset is a consistent set of images representing 102 different categories of flowers common to the UK. This dataset is often used for image classification tasks and research.\nDataset Details\n\nNumber of Classes: 102\nTotal Number of Images: 8189\nImages per Class: Varies from 40 to 258\nSize of Images: All images are resized to 500x500 pixels most of them same range if not squared image.\n\nClass Distribution\nThe dataset has a varied distribution of images across classes. Some classes have as few as 40 images, while others have as many as 258 images.\nData Split\nThe dataset is typically split into training, validation, and test sets. The recommended split is:\n\nTraining Set: 10 images per class\nValidation Set: 10 images per class\nTest Set: Remaining images\n\nUsage\nThis dataset is commonly used for image classification tasks, particularly for fine-grained classification tasks where the categories to be distinguished are very similar to each other.\nDownload\nYou can find more information about the Flowers 102 Dataset on the official webpage hosted by the (Visual Geometry Group - University of Oxford). The page provides an overview of the dataset, including details about the categories and the number of images for each class. It also offers downloads for the dataset images, image segmentations, image labels, and data splits.\nAdditionally, you can find more resources and benchmarks related to the Flowers 102 Dataset on\n(Papers With Code),  (TensorFlow Datasets), and (Kaggle)."},"projects/index":{"title":"Side Projects","links":[],"tags":[],"content":""},"resources/Study-Notes/Computer-Vision/index":{"title":"Computer Vision","links":[],"tags":["Technical","Writing","computer-vision","Study","Notes"],"content":""},"resources/Study-Notes/NLP/Transformers":{"title":"Transformer","links":[],"tags":["NLP","LLM","Transformers","Study-Notes"],"content":"Attention Is All You Need-Transformer\n\nModel\n\n\nA Transformer is an encoder-decoder model architecture that uses the attention mechanism.\nMassive advantage over RNN based encoder-decoder architecture since it allows to:\n\nBenefiting from the GPU/TPU parallelization.\nLarger amount of data processed in the same amount of time.\nprocess all tokens at once!\n\n\nThe Encoder / Decoder components are stack of Encoders and Decoders respectively.\n\nThe paper that introduced Transformers stacked 6 of each on top of each other respectively.\n\n\nThe models were built using attention mechanism at the core.\n\nbecause of the architecture, the attention mechanism helps improve the performance of machine translation applications.\n\n\n"},"resources/Study-Notes/NLP/index":{"title":"NLP","links":[],"tags":["Technical","Writing","Blog","NLP","LLM"],"content":""},"resources/Study-Notes/Spectrograms":{"title":"Spectrograms","links":[],"tags":["Frequency-Time","Study-Notes"],"content":"The Spectrogram and The Gabor Transform\nSummary\nThe Gabor transform is used to compute the spectrogram, which shows the frequency content and temporal information of a signal. It is useful for analyzing Brain Activity Signal, EEG, audio signals and classifying sounds.\nGabor Transform\nThe Gabor transform of a signal ( x(t) ) is defined by this formula:\nGx‚Äã(œÑ,œâ)=‚à´‚àí‚àû‚àû‚Äãx(t)e‚àíœÄ(t‚àíœÑ)2e‚àíjœâtdt\nIn this equation:\n\n( x(t) ) is the signal to be transformed.\nThe term : e‚àíœÄ(t‚àíœÑ)2represents a Gaussian window function, which gives higher weight to the signal near the time being analyzed.\nThe term : e‚àíjœât is the Fourier transform, which gives the frequency components of the signal.\nThe integral over all time (‚àí‚àû) to (‚àû) calculates the Fourier transform for each position of the Gaussian window, resulting in a time-frequency analysis.\n\nHighlights\n\nüéµ The Gabor transform computes the spectrogram, showing frequency and temporal information of a signal.\nüéπ The Fourier transform gives frequency components of a signal, but not their timing.\nüìä The Gabor transform combines frequency and temporal information to create a time-frequency plot.\nüåä The spectrogram is useful for classifying sounds, analyzing data, and identifying features in audio signals.\nüé∂ Shazam uses the spectrogram to match peaks in the power spectrum with known songs.\nüé∏ The spectrogram can help classify instruments and even differentiate musicians based on their playing style.\nüéπ The spectrogram of Beethoven‚Äôs sonata reveals the timing and intensity of each note.\n\nKey Insights\n\nüí° The Fourier transform provides frequency information, while the Gabor transform combines frequency and temporal information in the spectrogram.\nüí° The spectrogram is a valuable tool for analyzing audio signals, classifying sounds, and identifying features in data.\nüí° Shazam uses the spectrogram to match peaks in the power spectrum with known songs, even accounting for speed variations.\nüí° The spectrogram can be used to classify instruments and differentiate musicians based on playing styles.\nüí° The Gabor transform and spectrogram allow us to visualize the timing and intensity of notes in musical compositions.\nüí° The spectrogram is particularly useful for studying time-varying signals that are not perfectly periodic, like audio recordings.\nüí° Coding the spectrogram and working with time-frequency diagrams can provide further insights into audio signals and their characteristics.\n\n"},"resources/Study-Notes/index":{"title":"Study Notes","links":[],"tags":["Technical","Writing","Study","Notes"],"content":""},"resources/index":{"title":"Resources","links":[],"tags":["Technical","Writing","Study","Notes"],"content":""}}